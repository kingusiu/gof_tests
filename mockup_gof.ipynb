{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/00\n"
     ]
    }
   ],
   "source": [
    "import dadrah.analysis.root_plotting_util as rpu\n",
    "import dadrah.util.run_paths as runpa\n",
    "import dadrah.util.string_constants as stco\n",
    "import dadrah.selection.selection_util as seut\n",
    "import pofah.jet_sample as js\n",
    "import pofah.phase_space.cut_constants as cuts\n",
    "\n",
    "import pathlib\n",
    "import argparse\n",
    "import scipy as sci\n",
    "from scipy.stats import kstwo\n",
    "from scipy.stats import chi2\n",
    "\n",
    "import numpy as np\n",
    "import ROOT as rt\n",
    "import root_numpy as rtnp\n",
    "import uuid\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import root_numpy as rtnp\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.style.ROOT)\n",
    "import cmsstyle #mplhep for Python2 cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsecs_sig   = [0,20,40,60,80,100] # signal cross sections\n",
    "ae_run_n = 113\n",
    "#qr_run_n, qr_model_str, train_share = 8, 'dense_70pct_train', 0.7\n",
    "#qr_run_n, qr_model_str, train_share = 9, 'dense_50pct_train', 0.5\n",
    "#qr_run_n, qr_model_str, train_share = 7, 'dense_polyfit', 0.2\n",
    "qr_run_n, qr_model_str, train_share = 31, 'dense_kfold', 0\n",
    "#quantiles = [0.0, 0.3, 0.5, 0.7, 0.9, 0.99]\n",
    "quantiles = [0.0, 0.3, 0.5, 0.7, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsec_train = 0 # signal cross section used when training the QR\n",
    "mX = 3.5\n",
    "sample_id_qcd = 'qcdSigAllTest'+str(int((1-train_share)*100))+'pct' if train_share else 'qcdSigAll'\n",
    "sample_id_sig = 'GtoWW35naReco'\n",
    "sample_ids = [sample_id_qcd, sample_id_sig]\n",
    "quant_tmplt_idx = 0\n",
    "quant_tmplt = quantiles[quant_tmplt_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = runpa.RunPaths(in_data_dir=stco.dir_path_dict['base_dir_qr_selections'], in_data_names=stco.file_name_path_dict, out_data_dir=stco.dir_path_dict['base_dir_qr_analysis'])\n",
    "path_ext_dict = {'vae_run': str(ae_run_n), 'qr_run': str(qr_run_n), 'sig': sample_id_sig, 'xsec': str(int(xsec_train)), 'loss': 'rk5_05'}\n",
    "paths.extend_in_path_data(path_ext_dict)\n",
    "paths.extend_out_path_data({**path_ext_dict, 'hypothesis_testing': None})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fixed params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usign luminosity 63.98123745704467\n",
      "Scale QCD with 1.0\n",
      "Scale signal with 0.0006582093252100682\n"
     ]
    }
   ],
   "source": [
    "# Rescaling the inputs to lumi\n",
    "qcd_xsec         = 8730000.0 # Crossection in fb             \n",
    "qcd_gen_events   = 134366091.0+199435365.0+90490645.0+134264102.0 #(all generated QCD)\n",
    "sig_xsec_default = 10. #In units of fb (10 fb == 0.01 pb) \n",
    "sig_gen_events   = 972050.0 #(all generated signal)\n",
    "lumi             = qcd_gen_events/qcd_xsec # assuming 64/fb since qcd_gen_events/8730000 ~ 64\n",
    "\n",
    "scale_qcd = qcd_xsec*lumi/qcd_gen_events\n",
    "scale_sig = (1-train_share)*sig_xsec_default*lumi/sig_gen_events #X% already used for quantile regression, must be removed\n",
    "print('Usign luminosity {}'.format(lumi))\n",
    "print('Scale QCD with {}'.format(scale_qcd))\n",
    "print('Scale signal with {}'.format(scale_sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'template_q_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-73c29aea6abd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# efficiencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mquantiles_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantiles\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0meffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquantiles_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemplate_q_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquantiles_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemplate_q_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0meffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meffs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0meffs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# efficiency-test-quantiles/efficiency-template-quantile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'template_q_idx' is not defined"
     ]
    }
   ],
   "source": [
    "# efficiencies\n",
    "quantiles_tmp = np.asarray(quantiles+[1.])\n",
    "effs = (quantiles_tmp)[template_q_idx+1:] - (quantiles_tmp)[template_q_idx:-1]\n",
    "effs = effs[1:]/effs[0] # efficiency-test-quantiles/efficiency-template-quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 35\n",
    "min_mjj = 1600.\n",
    "max_mjj = 5200.\n",
    "expo_binning = False\n",
    "dijet_binning = True\n",
    "\n",
    "\n",
    "if dijet_binning:\n",
    "    bin_edges = np.array([1200, 1255, 1320, 1387, 1457, 1529,\n",
    "                          1604, 1681, 1761, 1844, 1930, 2019, \n",
    "                          2111, 2206, 2305, 2406, 2512, 2620, \n",
    "                          2733, 2849, 2969, 3093, 3221, 3353,\n",
    "                          3490, 3632, 3778, 3928, 4084, 4245, \n",
    "                          4411, 4583, 4760, 4943, 5132, 5327]).astype('float')\n",
    "elif expo_binning:\n",
    "    x_shift = 3\n",
    "    lin_bins = np.linspace(0.,1.,n_bins)\n",
    "    exp_bins = lin_bins/(np.exp(-lin_bins+x_shift)/np.exp(x_shift-1))\n",
    "    bin_edges = exp_bins*(max_mjj-min_mjj)+min_mjj\n",
    "    \n",
    "else: # simple linear binning\n",
    "    bin_edges = np.array(np.linspace(1200., max_mjj, n_bins).tolist()).astype('float') #100 GeV binning. Stop at 5600! Fit fails if going to 6800\n",
    "    \n",
    "n_bins = len(bin_edges)-1\n",
    "max_bin = bin_edges[-1]\n",
    "min_bin = bin_edges[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_selection_data(quantiles, ae_run_n, qr_run_n, sample_id_sig, xsec_train, paths):\n",
    "    samples = {}\n",
    "\n",
    "    for sample_id in sample_ids:\n",
    "        samples[sample_id] = js.JetSample.from_input_file(sample_id, paths.in_file_path(sample_id), **cuts.signalregion_cuts)\n",
    "        \n",
    "    # qcd raw data\n",
    "    samples_ortho_quantiles_qcd = seut.divide_sample_into_orthogonal_quantiles(samples[sample_id_qcd], quantiles[1:])\n",
    "    mjj_vals_qcd = [sample_ortho['mJJ'] for sample_ortho in samples_ortho_quantiles_qcd]\n",
    "    # signal raw data\n",
    "    samples_ortho_quantiles_sig = seut.divide_sample_into_orthogonal_quantiles(samples[sample_id_sig], quantiles[1:])\n",
    "    mjj_vals_sig = [sample_ortho['mJJ'] for sample_ortho in samples_ortho_quantiles_sig]\n",
    "    \n",
    "    return mjj_vals_qcd, mjj_vals_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_selection_data_to_numpy_hist(quantiles, ae_run_n, qr_run_n, sample_id_sig, xsec_train, bin_edges, paths):\n",
    "    \n",
    "    mjj_vals_qcd, mjj_vals_sig = read_raw_selection_data(quantiles, ae_run_n, qr_run_n, sample_id_sig, xsec_train, paths)\n",
    "    \n",
    "    datas_qcd = {}\n",
    "    datas_sig = {}\n",
    "    \n",
    "    # qcd histogram data\n",
    "    for mjj, q in zip(mjj_vals_qcd[template_q_idx:], quantiles[template_q_idx:]):\n",
    "        counts, _, _ = plt.hist(mjj, bins=bin_edges)\n",
    "        datas_qcd[q] = counts\n",
    "        \n",
    "    # sig histogram data\n",
    "    for mjj, q in zip(mjj_vals_sig[template_q_idx:], quantiles[template_q_idx:]):\n",
    "        counts, _, _ = plt.hist(mjj, bins=bin_edges)\n",
    "        datas_sig[q] = counts\n",
    "    \n",
    "    # make signal injection dataset for all signal xsecs\n",
    "    histos_data_inj = {}\n",
    "    \n",
    "     # for each quantile\n",
    "    for quant in quantiles[template_q_idx:]:\n",
    "        \n",
    "        histos_data_inj_quant = {}\n",
    "        \n",
    "        # for each signal cross section (each signal injection value)\n",
    "        for xsec in xsecs_sig:\n",
    "        \n",
    "            # get qcd data\n",
    "            dat = datas_qcd[quant]\n",
    "            # add signal data according to cross section\n",
    "            scale_xsec_sig = xsec/sig_xsec_default\n",
    "            sig_inj = datas_sig[quant]*scale_sig*scale_xsec_sig\n",
    "            # TODO: sample sig bin height from poisson?\n",
    "            \n",
    "            histos_data_inj_quant[xsec] = dat + sig_inj\n",
    "            \n",
    "        histos_data_inj[quant] = histos_data_inj_quant\n",
    "            \n",
    "    return bin_edges[:-1], histos_data_inj     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_centers, hist_data = read_raw_selection_data_to_numpy_hist(quantiles, ae_run_n, qr_run_n, sample_id_sig, xsec_train, bin_edges, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-sample test with poisson likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_log_likelihood(observed, expected): # bin_h_exp must be already scaled to efficiency\n",
    "    like = sci.stats.poisson.pmf(k=observed.astype('int'), mu=expected.astype('int'))\n",
    "    return np.sum(-np.log(like), axis=-1) # calculating negative log like (the smaller p, the larger -log like => test for right tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_value_from_toys(toy_likelihoods, obs_likelihood):\n",
    "    return sum(toy_likelihoods > obs_likelihood)/float(len(toy_likelihoods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods_per_quantile = {}\n",
    "expected_per_xse = hist_data[quant_tmplt]\n",
    "\n",
    "for quant_test, eff in zip(quantiles[quant_tmplt_idx+1:], effs):\n",
    "    \n",
    "    likelihoods_per_xsec = {}\n",
    "    observed_per_xsec = hist_data[quant_test]\n",
    "    \n",
    "    for xsec in xsecs_sig:\n",
    "        observed = observed_per_xsec[xsec]\n",
    "        expected = expected_per_xsec[xsec]*eff\n",
    "        likelihood = poisson_log_likelihood(observed, expected)\n",
    "        likelihoods_per_xsec[xsec] = likelihood\n",
    "        print('quant {}, xsec {}: likelihood {}'.format(quant_test,xsec,likelihood))\n",
    "    \n",
    "    likelihoods_per_quantile[quant_test] = likelihoods_per_xsec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute toys for null-test-statistic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_n = int(1e5)\n",
    "toy_likes_per_xsec = {}\n",
    "for xsec in xsecs_sig:\n",
    "    # take expected bin heights as mu\n",
    "    expected = expected_per_xsec[xsec]\n",
    "    toy = sci.stats.poisson.rvs(expected, size=(toy_n,len(expected)))\n",
    "    toy_likes = poisson_log_likelihood(toy, expected)\n",
    "    toy_likes_per_xsec[xsec] = toy_likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute and plot p-value under toy null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quantile, obs_likelihoods_per_xsec in likelihoods_per_quantile.items():\n",
    "    fig, axs = plt.subplots(1,len(xsecs_sig[::2]),sharey=True,figsize=(18,4))\n",
    "    for xsec, ax in zip(xsecs_sig[::2], axs.flat):\n",
    "        toy_likes = toy_likes_per_xsec[xsec]\n",
    "        obs_like = obs_likelihoods_per_xsec[xsec]\n",
    "        pval = p_value_from_toys(toy_likes, obs_like)\n",
    "        _ = ax.hist(toy_likes, bins=100)\n",
    "        ax.vlines(obs_like, color='r', ymin=0, ymax=3e3, ls=':',lw=3)\n",
    "        ax.set_title('xsec {}'.format(xsec), fontsize=16)\n",
    "    fig.suptitle('quantile {}'.format(quantile), fontsize=19)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
